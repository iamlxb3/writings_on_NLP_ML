## Background 

"Attention" related architecture has replaced RNNs as a 'default choice' for seq-to-seq modeling, however, the reason why attention outperforms other still remain unclear. My intention here is to uncover the underlying reasons.

[] argues transformer networks capture longer-range linguistic structure better than LSTM.

## Reference

[1]: http://jalammar.github.io/illustrated-gpt2/ "very good resources for transformer & attention"

