## User Behavior Modeling

- Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction (2019)
- Findings of the Association for Computational Linguistics PTUM: Pre-training User Model from Unlabeled User Behaviors via Self-supervision (2020)
- On the Effectiveness of Self-supervised Pre-training for Modeling User Behavior Sequences (2020)
- What to do next: Modeling user behaviors by Time-LSTM (2017)
- Multi-site User Behavior Modeling and Its Application in Video Recommendation (2017)
- ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation (2018)
- A temporal context-aware model for user behavior modeling in social media systems (2014)
- Behavior sequence transformer for e-commerce recommendation in Alibaba (2019)

## Pre-training Applications

- [nlp] DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations (2020)
- [nlp] CERT: Contrastive self-supervised learning for language understanding (2020)
- [nlp] Pre-trained models for natural language processing: A survey (2020)
- [nlp] Pre-training with contrastive sentence objectives improves discourse performance of language models (2020)
- [nlp] Improving Language Understanding by Generative Pre-Training (2020)
- [nlp] Local Self-Attention over Long Text for Efficient Document Retrieval
- [nlp] InterBERT: An Effective Multi-Modal Pretraining Approach via Vision-and-Language Interaction
- [speech] Unsupervised pre-training for sequence to sequence speech recognition (2019)
- [music] Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions (2020)
- [music] LakhNES: Improving multi-instrumental music generation with cross-domain pre-training
- [music] Music transformer: Generating music with long-term structure
- [video] Pixelwise Deep Sequence Learning for Moving Object Detection (2019)
- [code] Commit2Vec: Learning distributed representations of code changes (2019)
- [bio] Novel transformer networks for improved sequence labeling in genomics

## General Pre-training & self-training & contrastive learning & multi-view learning

- [pre-training] Train no evil: Selective masking for task-guided pre-training (2020)
- [pre-training] A Survey on Self-supervised Pre-training for Sequential Transfer Learning in Neural Networks (2020)
- [pre-training] An analysis of unsupervised pre-training in light of recent advances (2020)
- [pre-training] Why does unsupervised pre-training help deep learning?
- [contrastive learning] Exploring Simple Siamese Representation Learning (2020)
- [contrastive learning] A theoretical analysis of contrastive unsupervised representation learning (2019)
- [multi-view] A Survey of Multi-View Representation Learning

## Time embedding

- INDUCTIVE REPRESENTATION LEARNING ON TEMPORAL GRAPHS
- Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA
- LEARNING A SPATIO-TEMPORAL EMBEDDING FOR VIDEO INSTANCE SEGMENTATION
- SOM-VAE: INTERPRETABLE DISCRETE REPRESENTATION LEARNING ON TIME SERIES
- Unsupervised learning of action classes with continuous temporal embedding
- Self-attention with Functional Time Representation Learning
- Learning Temporal Embeddings for Complex Video Analysis

## Sequence representation

- A Survey on Contextual Embeddings



Person

---

**Title: HiTANet: Hierarchical Time-Aware Attention Networks for Risk Prediction on Electronic Health Records**

ID: 8d117fc86691455c1674baa7c550a3d2443b1870

Has been read 0 times.

Problem:  

Dataset: 

Contribution:

1. The authors propose to use time aware attention in two ways. First, to use it in local attention with diagnosis code. They also constructed pure time contexts, on which attention is applied to acquire a global time attention.

   Local time aware:

   ![image-20201112111047946](https://tva1.sinaimg.cn/large/0081Kckwly1gkm7tu9x9rj30ha0fr0vo.jpg)

   Global Time aware:

   ![image-20201112111230242](https://tva1.sinaimg.cn/large/0081Kckwly1gkm7tsm5v6j30hl0nfdlf.jpg)



Novelty:

Merit: 

Weakness: 

Other take aways:

---

