Ensemble learning: A survey

> Consensus clustering has been shown to be very effective in discovering biological meaningful clusters in gene expression data (Kiselev et al., 2017; Monti et al., 2003; Verhaak et al., 2010), video shot segmentation (Chang, Lee, Hong, & Archibald, 2008; Zheng, Zhang, & Li, 2012), online event detection (Wu et al., 2017), and cyber security (Liu & Lam, 2012).

>Clustering ensembles consist of two main stages, similar to those of supervised ensemble models: generation and integration (Vega-Pons & Ruiz-Shulcloper, 2011). 



---

Clustering ensemble selection considering quality and diversity

> Since the properness of any partition Ci is determined by all of the data points, the goodness function gj ? (Ci, D) depends not only on both the cluster Ci, but also on the entire dataset D. The stability as a measure of cluster goodness has been widely used in the literature (Lange et al. 2004; Alizadeh et al. 2011a, b). A stable cluster is the one that has a high likelihood of repetition across multiple executions of a clustering algorithm. Stable clusters are usually preferred, since they are robust with respect to minor changes in the dataset (Law et al. 2004).



---

Enhancement of Short Text Clustering by Iterative Classification

> k-means-- [10] is a variation of k-means clustering, in which outliers are
> removed in each iteration of the k-means clustering before recomputing the cluster centers. To detect outliers, short texts are ranked in decreasing order using their distances to their nearest cluster centers and the d (parameter for defining the total number of outliers) most distant texts are considered as outliers and removed from the clusters so that the cluster centers will become less sensitive to outliers. This has been confirmed to improve the clustering performance.